{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4ca11cc6-0616-449e-9719-6511751dbf6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import time\n",
    "from sklearn import decomposition\n",
    "import pandas as pd\n",
    "import torch\n",
    "import PIL\n",
    "\n",
    "project_root = '/user_data/mmhender/featsynth/'\n",
    "texture_synth_root = os.path.join(project_root, 'texture_synthesis')\n",
    "\n",
    "# this is in the 'texture_synthesis' folder\n",
    "import utilities\n",
    "import model_spatial\n",
    "\n",
    "# from image_analysis import extract_resnet_features\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f2bd700-59c8-4154-8b3c-cc97723a7b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imfn = df['image_filename'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3b36ae5-cdd2-4de0-9d26-2b71adef88bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_image = utilities.preprocess_image(\n",
    "                    utilities.load_image(imfn)\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "257c5550-53c4-4749-9c8c-c77f63760946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 36.0610,  36.0610,  34.0610,  ..., -36.9390, -38.9390, -41.9390],\n",
       "          [ 32.0610,  32.0610,  29.0610,  ..., -36.9390, -37.9390, -40.9390],\n",
       "          [ 30.0610,  28.0610,  25.0610,  ..., -34.9390, -36.9390, -39.9390],\n",
       "          ...,\n",
       "          [-72.9390, -70.9390, -67.9390,  ..., -54.9390, -59.9390, -59.9390],\n",
       "          [-70.9390, -67.9390, -67.9390,  ..., -55.9390, -61.9390, -61.9390],\n",
       "          [-71.9390, -71.9390, -74.9390,  ..., -57.9390, -64.9390, -65.9390]],\n",
       "\n",
       "         [[ 64.2210,  63.2210,  60.2210,  ...,  11.2210,   8.2210,   5.2210],\n",
       "          [ 63.2210,  61.2210,  58.2210,  ...,  10.2210,   9.2210,   6.2210],\n",
       "          [ 62.2210,  59.2210,  56.2210,  ...,  11.2210,   9.2210,   6.2210],\n",
       "          ...,\n",
       "          [-33.7790, -26.7790, -15.7790,  ..., -62.7790, -66.7790, -66.7790],\n",
       "          [-30.7790, -22.7790, -17.7790,  ..., -63.7790, -69.7790, -69.7790],\n",
       "          [-31.7790, -26.7790, -26.7790,  ..., -63.7790, -69.7790, -71.7790]],\n",
       "\n",
       "         [[ 96.3200,  93.3200,  89.3200,  ..., -19.6800, -22.6800, -25.6800],\n",
       "          [ 94.3200,  91.3200,  86.3200,  ..., -20.6800, -21.6800, -24.6800],\n",
       "          [ 92.3200,  89.3200,  84.3200,  ..., -18.6800, -20.6800, -24.6800],\n",
       "          ...,\n",
       "          [-67.6800, -61.6800, -52.6800,  ..., -75.6800, -80.6800, -81.6800],\n",
       "          [-66.6800, -58.6800, -54.6800,  ..., -77.6800, -82.6800, -82.6800],\n",
       "          [-66.6800, -62.6800, -63.6800,  ..., -79.6800, -83.6800, -85.6800]]]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b81d3a1b-4303-481a-966e-736dd02bc418",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[220, 217, 213, ..., 104, 101,  98],\n",
       "        [218, 215, 210, ..., 103, 102,  99],\n",
       "        [216, 213, 208, ..., 105, 103,  99],\n",
       "        ...,\n",
       "        [ 56,  62,  71, ...,  48,  43,  42],\n",
       "        [ 57,  65,  69, ...,  46,  41,  41],\n",
       "        [ 57,  61,  60, ...,  44,  40,  38]],\n",
       "\n",
       "       [[181, 180, 177, ..., 128, 125, 122],\n",
       "        [180, 178, 175, ..., 127, 126, 123],\n",
       "        [179, 176, 173, ..., 128, 126, 123],\n",
       "        ...,\n",
       "        [ 83,  90, 101, ...,  54,  50,  50],\n",
       "        [ 86,  94,  99, ...,  53,  47,  47],\n",
       "        [ 85,  90,  90, ...,  53,  47,  45]],\n",
       "\n",
       "       [[140, 140, 138, ...,  67,  65,  62],\n",
       "        [136, 136, 133, ...,  67,  66,  63],\n",
       "        [134, 132, 129, ...,  69,  67,  64],\n",
       "        ...,\n",
       "        [ 31,  33,  36, ...,  49,  44,  44],\n",
       "        [ 33,  36,  36, ...,  48,  42,  42],\n",
       "        [ 32,  32,  29, ...,  46,  39,  38]]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "474ef124-ef7d-4c20-b28a-dcb6ebd18c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import PIL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "976981c3-a316-4ac2-92ca-2c9b297ccce5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 36.0610,  36.0610,  34.0610,  ..., -36.9390, -38.9390, -41.9390],\n",
       "          [ 32.0610,  32.0610,  29.0610,  ..., -36.9390, -37.9390, -40.9390],\n",
       "          [ 30.0610,  28.0610,  25.0610,  ..., -34.9390, -36.9390, -39.9390],\n",
       "          ...,\n",
       "          [-72.9390, -70.9390, -67.9390,  ..., -54.9390, -59.9390, -59.9390],\n",
       "          [-70.9390, -67.9390, -67.9390,  ..., -55.9390, -61.9390, -61.9390],\n",
       "          [-71.9390, -71.9390, -74.9390,  ..., -57.9390, -64.9390, -65.9390]],\n",
       "\n",
       "         [[ 64.2210,  63.2210,  60.2210,  ...,  11.2210,   8.2210,   5.2210],\n",
       "          [ 63.2210,  61.2210,  58.2210,  ...,  10.2210,   9.2210,   6.2210],\n",
       "          [ 62.2210,  59.2210,  56.2210,  ...,  11.2210,   9.2210,   6.2210],\n",
       "          ...,\n",
       "          [-33.7790, -26.7790, -15.7790,  ..., -62.7790, -66.7790, -66.7790],\n",
       "          [-30.7790, -22.7790, -17.7790,  ..., -63.7790, -69.7790, -69.7790],\n",
       "          [-31.7790, -26.7790, -26.7790,  ..., -63.7790, -69.7790, -71.7790]],\n",
       "\n",
       "         [[ 96.3200,  93.3200,  89.3200,  ..., -19.6800, -22.6800, -25.6800],\n",
       "          [ 94.3200,  91.3200,  86.3200,  ..., -20.6800, -21.6800, -24.6800],\n",
       "          [ 92.3200,  89.3200,  84.3200,  ..., -18.6800, -20.6800, -24.6800],\n",
       "          ...,\n",
       "          [-67.6800, -61.6800, -52.6800,  ..., -75.6800, -80.6800, -81.6800],\n",
       "          [-66.6800, -58.6800, -54.6800,  ..., -77.6800, -82.6800, -82.6800],\n",
       "          [-66.6800, -62.6800, -63.6800,  ..., -79.6800, -83.6800, -85.6800]]]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "im = PIL.Image.fromarray(np.moveaxis(image_data[ii].astype(np.uint8),[0],[2]))\n",
    "\n",
    "target_image = utilities.preprocess_image(im)\n",
    "target_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "465bdf02-3f6a-47ee-9bd2-fc88715d5b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading images from /user_data/mmhender/featsynth/features/raw/images_expt1_intactonly_preproc.npy\n"
     ]
    }
   ],
   "source": [
    "debug=True\n",
    "image_set_name = 'images_expt1_intactonly'\n",
    "\n",
    "feat_path = os.path.join(project_root, 'features', 'gram_matrices')\n",
    "    \n",
    "if debug:\n",
    "    feat_path = os.path.join(feat_path,'DEBUG')\n",
    "\n",
    "if not os.path.exists(feat_path):\n",
    "    os.makedirs(feat_path)\n",
    "\n",
    "# load images to process\n",
    "folder_images = os.path.join(project_root, 'features','raw')\n",
    "image_data_filename = os.path.join(folder_images, '%s_preproc.npy'%(image_set_name))\n",
    "print('loading images from %s'%image_data_filename)\n",
    "image_data = np.load(image_data_filename)\n",
    "n_images = image_data.shape[0]\n",
    "\n",
    "# stuff about the model to get gram matrices from\n",
    "layers_process = ['pool1','pool2','pool3','pool4']\n",
    "            \n",
    "model_path = os.path.join(texture_synth_root, 'models','VGG19_normalized_avg_pool_pytorch')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "313686c1-1050-469c-9d5c-c2191ec34bb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 0.01102 s to preproc image for synthesis\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for ii in range(1):\n",
    "\n",
    "    st = time.time()\n",
    "    \n",
    "    # get image into format the model is expecting\n",
    "    im = PIL.Image.fromarray(np.moveaxis(image_data[ii].astype(np.uint8),[0],[2]))\n",
    "    target_image = utilities.preprocess_image(im)\n",
    "\n",
    "    elapsed = time.time() - st\n",
    "    print('took %.5f s to preproc image for synthesis'%elapsed)\n",
    "\n",
    "    net = model_spatial.Model(model_path, device, target_image, \\\n",
    "                                                      important_layers=layers_process, \\\n",
    "                                                      spatial_weights_list = None,\\\n",
    "                                                      do_sqrt = True)\n",
    "    \n",
    "    gram_mats = net.gram_loss_hook.target_gram_matrices\n",
    "\n",
    "    for li, gm in enumerate(gram_mats):\n",
    "\n",
    "        gram_features[li][ii,:] = gm.detach().cpu().numpy().ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f5927eb9-da8d-4ece-8d03-4b04f31b53ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4096, 16384, 65536, 262144]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[np.prod(gm.shape) for gm in gram_mats]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e19f4b2a-76d4-4a59-b1eb-b9fdcef85ab4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_grams = net.gram_loss_hook.target_gram_matrices\n",
    "len(target_grams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
